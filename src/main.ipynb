{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regex_to_NFA import NFA_CLASS,NFA\n",
    "from NFA_to_DFA import DFA_CLASS,State,Edge\n",
    "import json\n",
    "from parser_classes import EPSILON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_nfa(json_path: str) -> NFA:\n",
    "    \"\"\"\n",
    "    This function is used to convert the JSON data to NFA\n",
    "    params:\n",
    "        json_path: str: Path to the JSON file\n",
    "    return:\n",
    "        NFA: NFA object\n",
    "    \"\"\"\n",
    "    # read the JSON file\n",
    "    with open(json_path, \"r\") as file:\n",
    "        json_data = json.load(file)\n",
    "    # create the NFA object\n",
    "    nfa = NFA(None, None, [], [])\n",
    "    # create the states\n",
    "    states = []\n",
    "    acceping_index = 0\n",
    "    for state in json_data:\n",
    "        if state == \"startingState\":\n",
    "            continue\n",
    "        if json_data[state][\"isTerminatingState\"] == True:\n",
    "            acceping_index = int(state[1:])\n",
    "        states.append(State(name=state))\n",
    "    nfa.states = states\n",
    "    # set the start state becuase the name of the starting states is the index of it inside the states array\n",
    "    nfa.start = states[int(json_data[\"startingState\"][1:])]\n",
    "    # set the accept state\n",
    "    nfa.accept = states[acceping_index]\n",
    "    # create the transitions\n",
    "    transitions = []\n",
    "    for state in json_data:\n",
    "        if state == \"startingState\":\n",
    "            continue\n",
    "        for transition in json_data[state]:\n",
    "            if transition == \"isTerminatingState\":\n",
    "                continue\n",
    "\n",
    "            # check if the transition is epsilon or not\n",
    "            if transition.startswith(\"epsilon\"):\n",
    "                transitions.append(\n",
    "                    Edge(\n",
    "                        states[int(state[1:])],\n",
    "                        states[int(json_data[state][transition][1:])],\n",
    "                        {EPSILON},\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                transitions.append(\n",
    "                    Edge(\n",
    "                        states[int(state[1:])],\n",
    "                        states[int(json_data[state][transition][1:])],\n",
    "                        {transition},\n",
    "                    )\n",
    "                )\n",
    "    nfa.transitions = transitions\n",
    "    return nfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the inputs from the tansitions in the nfa\n",
    "def get_inputs(transitions):\n",
    "    \"\"\"_summary_\n",
    "    This function is used to get the inputs from the transitions in the NFA\n",
    "    Args:\n",
    "        transitions (List): List of transitions in the NFA (List of Edges)\n",
    "    Returns:\n",
    "        List: List of unique inputs in the NFA\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    for transition in transitions:\n",
    "        temp_in = list(transition.characters)[0]\n",
    "        if temp_in not in inputs and temp_in != EPSILON:\n",
    "            inputs.append(temp_in)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex to NFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex = \"[A-Za-z]+[0-9]*\"\n",
    "# regex = \"ab*c+de?(f|g|h)|mr|n|[p-qs0-9]\"\n",
    "# regex = \"ab\"\n",
    "regex = \"(A|B)+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'str' object has no attribute 'get'\n",
      "Error: Visualization failed\n",
      "Error: 'str' object has no attribute 'items'\n",
      "Error: Visualization failed\n"
     ]
    }
   ],
   "source": [
    "nfa = NFA_CLASS(regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NFA to DFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deserialize the json file to get the nfa\n",
    "deserialized_nfa = json_to_nfa(\"./nfa.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = get_inputs(deserialized_nfa.transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = DFA_CLASS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current State:  {State(name='S6'), State(name='S0'), State(name='S2'), State(name='S4')}\n",
      "Input: A, Move: {State(name='S2'), State(name='S5'), State(name='S7'), State(name='S0'), State(name='S1'), State(name='S4'), State(name='S6')}\n",
      "Input: B, Move: {State(name='S2'), State(name='S3'), State(name='S5'), State(name='S7'), State(name='S0'), State(name='S4'), State(name='S6')}\n",
      "Current State:  {State(name='S2'), State(name='S5'), State(name='S7'), State(name='S0'), State(name='S1'), State(name='S4'), State(name='S6')}\n",
      "Input: A, Move: {State(name='S2'), State(name='S5'), State(name='S7'), State(name='S0'), State(name='S1'), State(name='S4'), State(name='S6')}\n",
      "Input: B, Move: {State(name='S2'), State(name='S3'), State(name='S5'), State(name='S7'), State(name='S0'), State(name='S4'), State(name='S6')}\n",
      "Current State:  {State(name='S2'), State(name='S3'), State(name='S5'), State(name='S7'), State(name='S0'), State(name='S4'), State(name='S6')}\n",
      "Input: A, Move: {State(name='S2'), State(name='S5'), State(name='S7'), State(name='S0'), State(name='S1'), State(name='S4'), State(name='S6')}\n",
      "Input: B, Move: {State(name='S2'), State(name='S3'), State(name='S5'), State(name='S7'), State(name='S0'), State(name='S4'), State(name='S6')}\n"
     ]
    }
   ],
   "source": [
    "dfa.subset_construction(deserialized_nfa, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start State:  S6,S0,S2,S4\n",
      "Accepting States:  ['S2,S5,S7,S0,S1,S4,S6', 'S2,S3,S5,S7,S0,S4,S6']\n",
      "Non Accepting States:  ['S6,S0,S2,S4']\n",
      "States:  ['S6,S0,S2,S4', 'S2,S5,S7,S0,S1,S4,S6', 'S2,S3,S5,S7,S0,S4,S6']\n",
      "Transitions: \n",
      "From: S6,S0,S2,S4, To: S2,S5,S7,S0,S1,S4,S6, Characters: {'A'}\n",
      "From: S6,S0,S2,S4, To: S2,S3,S5,S7,S0,S4,S6, Characters: {'B'}\n",
      "From: S2,S5,S7,S0,S1,S4,S6, To: S2,S5,S7,S0,S1,S4,S6, Characters: {'A'}\n",
      "From: S2,S5,S7,S0,S1,S4,S6, To: S2,S3,S5,S7,S0,S4,S6, Characters: {'B'}\n",
      "From: S2,S3,S5,S7,S0,S4,S6, To: S2,S5,S7,S0,S1,S4,S6, Characters: {'A'}\n",
      "From: S2,S3,S5,S7,S0,S4,S6, To: S2,S3,S5,S7,S0,S4,S6, Characters: {'B'}\n"
     ]
    }
   ],
   "source": [
    "dfa.print_dfa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start State:  S0\n",
      "Accepting States:  ['S1', 'S2']\n",
      "Non Accepting States:  ['S0']\n",
      "States:  ['S0', 'S1', 'S2']\n",
      "Transitions: \n",
      "From: S0, To: S1, Characters: {'A'}\n",
      "From: S0, To: S2, Characters: {'B'}\n",
      "From: S1, To: S1, Characters: {'A'}\n",
      "From: S1, To: S2, Characters: {'B'}\n",
      "From: S2, To: S1, Characters: {'A'}\n",
      "From: S2, To: S2, Characters: {'B'}\n"
     ]
    }
   ],
   "source": [
    "dfa.rename_dfa_states()\n",
    "dfa.print_dfa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa.dfa_to_json(\"dfa.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH\n",
      "Error: Visualization DFA failed\n"
     ]
    }
   ],
   "source": [
    "dfa.visualize_dfa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing DFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimized_dfa = dfa.minimize_dfa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new object of DFA_CLASS to use the minimized dfa\n",
    "minimized_dfa_obj = DFA_CLASS(minimized_dfa,inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start State:  S0\n",
      "Accepting States:  ['S1', 'S2']\n",
      "Non Accepting States:  ['S0']\n",
      "States:  ['S0', 'S1', 'S2']\n",
      "Transitions: \n",
      "From: S0, To: S1, Characters: {'A'}\n",
      "From: S0, To: S2, Characters: {'B'}\n",
      "From: S1, To: S1, Characters: {'A'}\n",
      "From: S1, To: S2, Characters: {'B'}\n",
      "From: S2, To: S1, Characters: {'A'}\n",
      "From: S2, To: S2, Characters: {'B'}\n"
     ]
    }
   ],
   "source": [
    "# print the minimized dfa\n",
    "minimized_dfa_obj.print_dfa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimized_dfa_obj.dfa_to_json(\"minimized_dfa.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH\n",
      "Error: Visualization DFA failed\n"
     ]
    }
   ],
   "source": [
    "minimized_dfa_obj.visualize_dfa(path=\"minimized_dfa.gv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "No more tokens to parse",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m input_regex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter the regular expression: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m nfa \u001b[38;5;241m=\u001b[39m \u001b[43mNFA_CLASS\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_regex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m deserialized_nfa \u001b[38;5;241m=\u001b[39m json_to_nfa(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./nfa.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m get_inputs(deserialized_nfa\u001b[38;5;241m.\u001b[39mtransitions)\n",
      "File \u001b[1;32mc:\\Users\\iiBesh00\\Desktop\\CMP42\\Compilers\\Assignment\\Compilers_Lab\\src\\regex_to_NFA.py:41\u001b[0m, in \u001b[0;36mNFA_CLASS.__init__\u001b[1;34m(self, regex)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nfa_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize()\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mAST_to_NFA()\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfa_to_json()\n",
      "File \u001b[1;32mc:\\Users\\iiBesh00\\Desktop\\CMP42\\Compilers\\Assignment\\Compilers_Lab\\src\\regex_to_NFA.py:108\u001b[0m, in \u001b[0;36mNFA_CLASS.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     94\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m    Function used to create the Abstract Syntax Tree (AST) of the regex using the following grammar\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m                            | LITERAL DASH LITERAL\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     expression, _ \u001b[38;5;241m=\u001b[39m \u001b[43mparse_regex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ast \u001b[38;5;241m=\u001b[39m expression\n",
      "File \u001b[1;32mc:\\Users\\iiBesh00\\Desktop\\CMP42\\Compilers\\Assignment\\Compilers_Lab\\src\\parser_classes.py:62\u001b[0m, in \u001b[0;36mparse_regex\u001b[1;34m(tokens, current_token)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_regex\u001b[39m(tokens, current_token):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    regex-expression -> or-expression\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_or\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_token\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\iiBesh00\\Desktop\\CMP42\\Compilers\\Assignment\\Compilers_Lab\\src\\parser_classes.py:69\u001b[0m, in \u001b[0;36mparse_or\u001b[1;34m(tokens, current_token)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03mor-expression -> sequence-expression (OR sequence-expression)*\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Get the left sequence expression\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m left, current_token \u001b[38;5;241m=\u001b[39m \u001b[43mparse_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Check if there are no more tokens\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_token \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens):\n",
      "File \u001b[1;32mc:\\Users\\iiBesh00\\Desktop\\CMP42\\Compilers\\Assignment\\Compilers_Lab\\src\\parser_classes.py:94\u001b[0m, in \u001b[0;36mparse_sequence\u001b[1;34m(tokens, current_token)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03msequence-expression -> quantified-expression (quantified-expression)*\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Get the left quantified expression\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m left, current_token \u001b[38;5;241m=\u001b[39m \u001b[43mparse_quantified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Check if there are no more tokens\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_token \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens):\n",
      "File \u001b[1;32mc:\\Users\\iiBesh00\\Desktop\\CMP42\\Compilers\\Assignment\\Compilers_Lab\\src\\parser_classes.py:119\u001b[0m, in \u001b[0;36mparse_quantified\u001b[1;34m(tokens, current_token)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03mquantified-expression -> base-expression (STAR | PLUS | QUESTION_MARK)?\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Get the left base expression\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m left, current_token \u001b[38;5;241m=\u001b[39m \u001b[43mparse_base\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Check if there are no more tokens\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_token \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens):\n",
      "File \u001b[1;32mc:\\Users\\iiBesh00\\Desktop\\CMP42\\Compilers\\Assignment\\Compilers_Lab\\src\\parser_classes.py:148\u001b[0m, in \u001b[0;36mparse_base\u001b[1;34m(tokens, current_token)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Check if there are no more tokens\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_token \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens):\n\u001b[1;32m--> 148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo more tokens to parse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    150\u001b[0m token \u001b[38;5;241m=\u001b[39m tokens[current_token]\n\u001b[0;32m    151\u001b[0m current_token \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mException\u001b[0m: No more tokens to parse"
     ]
    }
   ],
   "source": [
    "input_regex = input(\"Enter the regular expression: \")\n",
    "nfa = NFA_CLASS(input_regex)\n",
    "deserialized_nfa = json_to_nfa(\"./nfa.json\")\n",
    "inputs = get_inputs(deserialized_nfa.transitions)\n",
    "dfa = DFA_CLASS()\n",
    "dfa.subset_construction(deserialized_nfa, inputs)\n",
    "dfa.rename_dfa_states()\n",
    "# dfa.print_dfa()\n",
    "minimized_dfa = dfa.minimize_dfa()\n",
    "minimized_dfa_obj = DFA_CLASS(minimized_dfa,inputs)\n",
    "minimized_dfa_obj.rename_dfa_states()\n",
    "# minimized_dfa_obj.print_dfa()\n",
    "minimized_dfa_obj.dfa_to_json(\"minimized_dfa.json\")\n",
    "minimized_dfa_obj.visualize_dfa(path=\"minimized_dfa.gv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compiler_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
