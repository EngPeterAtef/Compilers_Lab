{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3sdWKYkFIjJ"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "<th>Name</th>\n",
        "<th>Section</th>\n",
        "<th>B.N</th>\n",
        "<th>ID</th>\n",
        "</tr>\n",
        "<tr>\n",
        "<th>Beshoy Morad Atya</th>\n",
        "<th>1</th>\n",
        "<th>19</th>\n",
        "<th>9202405</th>\n",
        "</tr>\n",
        "<tr>\n",
        "<th>Peter Atef Fathi</th>\n",
        "<th>1</th>\n",
        "<th>18</th>\n",
        "<th>9202395</th>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ks9A6hAlHO-5"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRNWlGtwHMbA",
        "outputId": "1724dacf-1b67-478c-c7bb-b12037ece234"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (0.20.3)\n"
          ]
        }
      ],
      "source": [
        "%pip install graphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "69cRg-nJFDFE"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import graphviz\n",
        "import re\n",
        "from enum import Enum, auto\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLJMibWvHyia"
      },
      "source": [
        "### Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wJaHXGkZHh6l"
      },
      "outputs": [],
      "source": [
        "EPSILON = 'epsilon'\n",
        "\n",
        "class TokenType(Enum):\n",
        "    OR = auto()                 # |\n",
        "    STAR = auto()               # *\n",
        "    PLUS = auto()               # +\n",
        "    QUESTION_MARK = auto()      # ?\n",
        "    OPEN_PAREN = auto()         # (\n",
        "    CLOSED_PAREN = auto()       # )\n",
        "    OPEN_SQ_BRACKET = auto()    # [\n",
        "    CLOSED_SQ_BRACKET = auto()  # ]\n",
        "    DASH = auto()               # -\n",
        "    LITERAL = auto()            # a-z A-Z 0-9\n",
        "    DOT = auto()                # .\n",
        "\n",
        "@dataclass\n",
        "class Token:\n",
        "    token_type: TokenType\n",
        "    string: str\n",
        "\n",
        "class AstNode:\n",
        "    pass\n",
        "\n",
        "@dataclass\n",
        "class OrAstNode(AstNode):\n",
        "    left: AstNode\n",
        "    right: AstNode\n",
        "\n",
        "@dataclass\n",
        "class SeqAstNode(AstNode):\n",
        "    left: AstNode\n",
        "    right: AstNode\n",
        "\n",
        "@dataclass\n",
        "class StarAstNode(AstNode):\n",
        "    left: AstNode\n",
        "\n",
        "@dataclass\n",
        "class PlusAstNode(AstNode):\n",
        "    left: AstNode\n",
        "\n",
        "@dataclass\n",
        "class QuestionMarkAstNode(AstNode):\n",
        "    left: AstNode\n",
        "\n",
        "@dataclass\n",
        "class LiteralCharacterAstNode(AstNode):\n",
        "    char: str\n",
        "\n",
        "\n",
        "def parse_regex(tokens, current_token):\n",
        "    \"\"\"\n",
        "    regex-expression -> or-expression\n",
        "    \"\"\"\n",
        "    return parse_or(tokens, current_token)\n",
        "\n",
        "def parse_or(tokens, current_token):\n",
        "    \"\"\"\n",
        "    or-expression -> sequence-expression (OR sequence-expression)*\n",
        "    \"\"\"\n",
        "    # Get the left sequence expression\n",
        "    left, current_token = parse_sequence(tokens, current_token)\n",
        "\n",
        "    # Check if there are no more tokens\n",
        "    if current_token >= len(tokens):\n",
        "        return left, current_token\n",
        "\n",
        "    # Final AST node\n",
        "    or_expression = left\n",
        "\n",
        "    # Loop through the rest of the tokens\n",
        "    while current_token < len(tokens) and tokens[current_token].token_type == TokenType.OR:\n",
        "        current_token += 1\n",
        "\n",
        "        # Get the right sequence expression\n",
        "        right, current_token = parse_sequence(tokens, current_token)\n",
        "\n",
        "        or_expression = OrAstNode(left=or_expression, right=right)\n",
        "\n",
        "    return or_expression, current_token\n",
        "\n",
        "def parse_sequence(tokens, current_token):\n",
        "    \"\"\"\n",
        "    sequence-expression -> quantified-expression (quantified-expression)*\n",
        "    \"\"\"\n",
        "    # Get the left quantified expression\n",
        "    left, current_token = parse_quantified(tokens, current_token)\n",
        "\n",
        "    # Check if there are no more tokens\n",
        "    if current_token >= len(tokens):\n",
        "        return left, current_token\n",
        "\n",
        "    # Final AST node\n",
        "    sequence_expression = left\n",
        "\n",
        "    # Loop through the rest of the tokens\n",
        "    while current_token < len(tokens) \\\n",
        "        and tokens[current_token].token_type != TokenType.OR \\\n",
        "        and tokens[current_token].token_type != TokenType.CLOSED_PAREN:\n",
        "\n",
        "        right, current_token = parse_quantified(tokens, current_token)\n",
        "\n",
        "        sequence_expression = SeqAstNode(left=sequence_expression, right=right)\n",
        "\n",
        "    return sequence_expression, current_token\n",
        "\n",
        "def parse_quantified(tokens, current_token):\n",
        "    \"\"\"\n",
        "    quantified-expression -> base-expression (STAR | PLUS | QUESTION_MARK)?\n",
        "    \"\"\"\n",
        "    # Get the left base expression\n",
        "    left, current_token = parse_base(tokens, current_token)\n",
        "\n",
        "    # Check if there are no more tokens\n",
        "    if current_token >= len(tokens):\n",
        "        return left, current_token\n",
        "\n",
        "    if tokens[current_token].token_type == TokenType.STAR:\n",
        "        current_token += 1\n",
        "        return StarAstNode(left=left), current_token\n",
        "\n",
        "    if tokens[current_token].token_type == TokenType.PLUS:\n",
        "        current_token += 1\n",
        "        return PlusAstNode(left=left), current_token\n",
        "\n",
        "    if tokens[current_token].token_type == TokenType.QUESTION_MARK:\n",
        "        current_token += 1\n",
        "        return QuestionMarkAstNode(left=left), current_token\n",
        "\n",
        "    return left, current_token\n",
        "\n",
        "def parse_base(tokens, current_token):\n",
        "    \"\"\"\n",
        "    base-expression -> LITERAL\n",
        "                    | DOT\n",
        "                    | OPEN_PAREN regex-expression CLOSED_PAREN\n",
        "                    | OPEN_SQ_BRACKET sq-bracket-content CLOSED_SQ_BRACKET\n",
        "    \"\"\"\n",
        "    # Check if there are no more tokens\n",
        "    if current_token >= len(tokens):\n",
        "        raise Exception(\"No more tokens to parse\")\n",
        "\n",
        "    token = tokens[current_token]\n",
        "    current_token += 1\n",
        "\n",
        "    if token.token_type == TokenType.LITERAL:\n",
        "        return LiteralCharacterAstNode(char=token.string), current_token\n",
        "\n",
        "    if token.token_type == TokenType.DOT:\n",
        "        # Epsilon because dot match any character\n",
        "        return LiteralCharacterAstNode(char='.'), current_token\n",
        "\n",
        "    if token.token_type == TokenType.OPEN_PAREN:\n",
        "        expression, current_token = parse_regex(tokens, current_token)\n",
        "\n",
        "        if current_token >= len(tokens):\n",
        "            raise Exception(\"No more tokens to parse\")\n",
        "\n",
        "        if tokens[current_token].token_type != TokenType.CLOSED_PAREN:\n",
        "            raise Exception(\"Expected closed parenthesis\")\n",
        "\n",
        "        current_token += 1\n",
        "        return expression, current_token\n",
        "\n",
        "    if token.token_type == TokenType.OPEN_SQ_BRACKET:\n",
        "        sq_bracket, current_token = parse_sq_bracket_content(tokens, current_token)\n",
        "\n",
        "        if current_token >= len(tokens):\n",
        "            raise Exception(\"No more tokens to parse\")\n",
        "\n",
        "        if tokens[current_token].token_type != TokenType.CLOSED_SQ_BRACKET:\n",
        "            raise Exception(\"Expected square bracket\")\n",
        "\n",
        "        current_token += 1\n",
        "        return sq_bracket, current_token\n",
        "\n",
        "def parse_sq_bracket_content(tokens, current_token):\n",
        "    \"\"\"\n",
        "    sq-bracket-content -> LITERAL\n",
        "                        | LITERAL DASH LITERAL\n",
        "    \"\"\"\n",
        "    # Check if there are no more tokens\n",
        "    if current_token >= len(tokens):\n",
        "        raise Exception(\"No more tokens to parse\")\n",
        "\n",
        "    chars = []\n",
        "    is_dash_reached = False\n",
        "\n",
        "    while current_token < len(tokens) \\\n",
        "        and tokens[current_token].token_type != TokenType.CLOSED_SQ_BRACKET:\n",
        "\n",
        "        if tokens[current_token].token_type == TokenType.DASH:\n",
        "            is_dash_reached = True\n",
        "        elif is_dash_reached:\n",
        "            if len(chars) == 0:\n",
        "                raise Exception(\"Expected at least one character\")\n",
        "\n",
        "            range_start = chars.pop()\n",
        "            range_end = tokens[current_token].string\n",
        "            chars.append((range_start, range_end))\n",
        "            is_dash_reached = False\n",
        "        else:\n",
        "            chars.append(tokens[current_token].string)\n",
        "\n",
        "        current_token += 1\n",
        "\n",
        "    if is_dash_reached:\n",
        "        raise Exception(\"Wrong range format\")\n",
        "\n",
        "    # Handle the cases like [a-z b 0-5] should be like (a-z | b | 0-5)\n",
        "    expression = None\n",
        "    for char in chars:\n",
        "        if isinstance(char, tuple):\n",
        "            if expression is None:\n",
        "                expression = LiteralCharacterAstNode(char=f\"{char[0]}-{char[1]}\")\n",
        "            else:\n",
        "                expression = OrAstNode(left=expression, right=LiteralCharacterAstNode(char=f\"{char[0]}-{char[1]}\"))\n",
        "        else:\n",
        "            if expression is None:\n",
        "                expression = LiteralCharacterAstNode(char=char)\n",
        "            else:\n",
        "                expression = OrAstNode(left=expression, right=LiteralCharacterAstNode(char=char))\n",
        "\n",
        "    return expression, current_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05-UC2AoH4Na"
      },
      "source": [
        "### Json Serializer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2fYktMS6H3q7"
      },
      "outputs": [],
      "source": [
        "class JsonSerialize:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def nfa_json_serialize(self, nfa):\n",
        "        \"\"\"_summary_\n",
        "\n",
        "        Args:\n",
        "            nfa (NFA): The NFA data object\n",
        "\n",
        "        Returns:\n",
        "            dict: dictionary of the json data\n",
        "        \"\"\"\n",
        "        json_data = {}\n",
        "\n",
        "        address_to_name = {}\n",
        "        from_state = {}\n",
        "\n",
        "        for _, state in enumerate(nfa.states):\n",
        "            name = f\"S{str(nfa.states.index(state))}\"\n",
        "            address_to_name[state] = name\n",
        "            from_state[name] = []\n",
        "\n",
        "        for transition in nfa.transitions:\n",
        "            # Get all the states that the transition goes to\n",
        "            from_state[address_to_name[transition.from_]].append(\n",
        "                {\"to\": address_to_name[transition.to_], \"char\": transition.characters}\n",
        "            )\n",
        "\n",
        "        # Extracting starting state\n",
        "        json_data[\"startingState\"] = address_to_name[nfa.start]\n",
        "\n",
        "        for _, state in enumerate(from_state):\n",
        "            epsilons = 1\n",
        "            json_data[state] = {\n",
        "                \"isTerminatingState\": state == address_to_name[nfa.accept],\n",
        "            }\n",
        "\n",
        "            transitions = {}\n",
        "            for transition in from_state[state]:\n",
        "                if transition[\"char\"] == \"epsilon\":\n",
        "                    transitions[f\"epsilon{epsilons}\"] = transition[\"to\"]\n",
        "                    epsilons += 1\n",
        "                else:\n",
        "                    transitions[transition[\"char\"]] = transition[\"to\"]\n",
        "\n",
        "            # Check if transitions is empty\n",
        "            if len(transitions) > 0:\n",
        "                json_data[state].update(transitions)\n",
        "\n",
        "        return json_data\n",
        "\n",
        "    def dfa_json_serialize(self, dfa):\n",
        "        \"\"\"_summary_\n",
        "        This function is used to serialize the DFA object to JSON format\n",
        "        Args:\n",
        "            dfa (DFA): the DFA data object\n",
        "        Returns:\n",
        "            dict: dictionary of the json data\n",
        "        \"\"\"\n",
        "        json_data = {}\n",
        "        # set the starting state in the json data\n",
        "        json_data[\"startingState\"] = dfa.start.name\n",
        "\n",
        "        # loop over the states in the DFA\n",
        "        for _, state in enumerate(dfa.states):\n",
        "            # loop over the transitions in the state\n",
        "            json_data[state.name] = {\n",
        "                \"isTerminatingState\": state in dfa.accept,\n",
        "            }\n",
        "            for transition in dfa.transitions:\n",
        "                if transition.from_ == state:\n",
        "                    json_data[state.name][\n",
        "                        \"\".join(list(transition.characters))\n",
        "                    ] = transition.to_.name\n",
        "        return json_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZlD9FL_IAiE"
      },
      "source": [
        "### Graph Visualizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eK9y9POjILPX"
      },
      "outputs": [],
      "source": [
        "class GraphVisualize:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def graph_visualize(self, name=\"graph\", json_data=None):\n",
        "        \"\"\"_summary_\n",
        "        Args:\n",
        "            name (str, optional): name of the file .gv. Defaults to \"graph_visualization.gv\".\n",
        "            json_data (dict): Dictionary of the json data. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            None: no return value\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if json_data is None:\n",
        "                print(\"Error: No data to visualize\")\n",
        "                return False\n",
        "            graph = graphviz.Digraph(engine=\"dot\")\n",
        "\n",
        "            for state, transitions in json_data.items():\n",
        "                if state == \"startingState\":\n",
        "                    graph.edge(\"Start\", transitions)\n",
        "                    continue\n",
        "\n",
        "                if transitions.get(\"isTerminatingState\", False):\n",
        "                    graph.node(state, shape=\"doublecircle\")\n",
        "                else:\n",
        "                    graph.node(state, shape=\"circle\")\n",
        "\n",
        "                for char, next_state in transitions.items():\n",
        "                    if char == \"isTerminatingState\":\n",
        "                        continue\n",
        "                    if \"epsilon\" in char:\n",
        "                        graph.edge(state, next_state, label=\"epsilon\")\n",
        "                    else:\n",
        "                        graph.edge(state, next_state, label=char)\n",
        "\n",
        "            graph.render(name, format=\"png\", cleanup=True)\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            return False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r89iPJ_0IXvT"
      },
      "source": [
        "### Regex to NFA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KiAnSSCEIZt6"
      },
      "outputs": [],
      "source": [
        "class State:\n",
        "    pass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Edge:\n",
        "    from_: State\n",
        "    to_: State\n",
        "    characters: str  # of chars (literals, epsilon)\n",
        "\n",
        "@dataclass\n",
        "class NFA:\n",
        "    start: State\n",
        "    accept: State\n",
        "\n",
        "    # All states of the NFA including the start and accept\n",
        "    states: list  # of State\n",
        "    transitions: list  # of Edge\n",
        "\n",
        "\n",
        "class NFA_CLASS:\n",
        "    def __init__(self, regex):\n",
        "        if not self.check_regex(regex):\n",
        "            raise ValueError(f\"Invalid regular expression: {regex}\")\n",
        "\n",
        "        self._regex = regex\n",
        "        self._tokens = []\n",
        "        self._ast = None\n",
        "        self._nfa = None\n",
        "        self._nfa_json = None\n",
        "\n",
        "        self.tokenize()\n",
        "        self.parse()\n",
        "        self.AST_to_NFA()\n",
        "        self.nfa_to_json()\n",
        "        self.nfa_visualize()\n",
        "\n",
        "    def check_regex(self, regex):\n",
        "        \"\"\"\n",
        "        Function used to check if the regex is valid or not\n",
        "        \"\"\"\n",
        "        try:\n",
        "            re.compile(regex)\n",
        "        except re.error:\n",
        "            print(f\"Invalid regular expression: {regex}\")\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def tokenize(self):\n",
        "        \"\"\"\n",
        "        Function used to tokenize the regex, and handle the escape char '\\'\n",
        "        that helps us to escape the special characters in regex | * + ? ( ) [ ] - .\n",
        "        \"\"\"\n",
        "        character_map = {\n",
        "            \"|\": TokenType.OR,\n",
        "            \"*\": TokenType.STAR,\n",
        "            \"+\": TokenType.PLUS,\n",
        "            \"?\": TokenType.QUESTION_MARK,\n",
        "            \"(\": TokenType.OPEN_PAREN,\n",
        "            \")\": TokenType.CLOSED_PAREN,\n",
        "            \"[\": TokenType.OPEN_SQ_BRACKET,\n",
        "            \"]\": TokenType.CLOSED_SQ_BRACKET,\n",
        "            \"-\": TokenType.DASH,\n",
        "            \".\": TokenType.DOT,\n",
        "        }\n",
        "        escape = \"\\\\\"\n",
        "\n",
        "        token_stream = []\n",
        "        prev_char = None\n",
        "\n",
        "        for char in self._regex:\n",
        "            if char == escape:\n",
        "                prev_char = char\n",
        "                continue\n",
        "\n",
        "            if char in character_map and prev_char != escape:\n",
        "                token_stream.append(Token(character_map[char], char))\n",
        "            else:\n",
        "                token_stream.append(Token(TokenType.LITERAL, char))\n",
        "\n",
        "            prev_char = char\n",
        "\n",
        "        self._tokens = token_stream\n",
        "\n",
        "    def parse(self):\n",
        "        \"\"\"\n",
        "        Function used to create the Abstract Syntax Tree (AST) of the regex using the following grammar\n",
        "\n",
        "        regex-expression        -> or-expression\n",
        "        or-expression           -> sequence-expression (OR sequence-expression)*\n",
        "        sequence-expression     -> quantified-expression (quantified-expression)*\n",
        "        quantified-expression   -> base-expression (STAR | PLUS | QUESTION_MARK)?\n",
        "        base-expression         -> LITERAL\n",
        "                                | DOT\n",
        "                                | OPEN_PAREN regex-expression CLOSED_PAREN\n",
        "                                | OPEN_SQ_BRACKET sq-bracket-content CLOSED_SQ_BRACKET\n",
        "        sq-bracket-content      -> LITERAL\n",
        "                                | LITERAL DASH LITERAL\n",
        "        \"\"\"\n",
        "        expression, _ = parse_regex(self._tokens, 0)\n",
        "        self._ast = expression\n",
        "\n",
        "    def construct_nfa(self, node):\n",
        "        if isinstance(node, OrAstNode):\n",
        "            return self.construct_or_nfa(node)\n",
        "        elif isinstance(node, SeqAstNode):\n",
        "            return self.construct_seq_nfa(node)\n",
        "        elif isinstance(node, StarAstNode) or isinstance(node, PlusAstNode):\n",
        "            return self.construct_star_plus_nfa(node)\n",
        "        elif isinstance(node, QuestionMarkAstNode):\n",
        "            return self.construct_question_mark_nfa(node)\n",
        "        elif isinstance(node, LiteralCharacterAstNode):\n",
        "            return self.construct_literal_class_nfa(node)\n",
        "\n",
        "    def construct_or_nfa(self, node):\n",
        "        # If we have regex A|B\n",
        "\n",
        "        # Get the NFAs representing A and B\n",
        "        left_nfa = self.construct_nfa(node.left)\n",
        "        right_nfa = self.construct_nfa(node.right)\n",
        "\n",
        "        # Create 2 new states, start and accept for the full NFA\n",
        "        start = State()\n",
        "        accept = State()\n",
        "\n",
        "        # Link the start state with both the start of A's NFA and B's NFA with an epsilon transitions\n",
        "        start_transition_1 = Edge(from_=start, to_=left_nfa.start, characters=EPSILON)\n",
        "        start_transition_2 = Edge(\n",
        "            from_=start, to_=right_nfa.start, characters=EPSILON\n",
        "        )\n",
        "\n",
        "        # Link the accept state with both the accept of A's NFA and B's NFA with an epsilon transitions\n",
        "        final_transition_1 = Edge(\n",
        "            from_=left_nfa.accept, to_=accept, characters=EPSILON\n",
        "        )\n",
        "        final_transition_2 = Edge(\n",
        "            from_=right_nfa.accept, to_=accept, characters=EPSILON\n",
        "        )\n",
        "\n",
        "        # Finalize the Or NFA and return it\n",
        "        states = [*left_nfa.states, *right_nfa.states, start, accept]\n",
        "        transitions = [\n",
        "            *left_nfa.transitions,\n",
        "            *right_nfa.transitions,\n",
        "            start_transition_1,\n",
        "            start_transition_2,\n",
        "            final_transition_1,\n",
        "            final_transition_2,\n",
        "        ]\n",
        "        return NFA(start, accept, states, transitions)\n",
        "\n",
        "    def construct_seq_nfa(self, node):\n",
        "        # If we have regex AB\n",
        "\n",
        "        # Get the NFAs representing A and B\n",
        "        left_nfa = self.construct_nfa(node.left)\n",
        "        right_nfa = self.construct_nfa(node.right)\n",
        "\n",
        "        # Link the two NFAs by connecting the accept state of A to the start state of B\n",
        "        final_transition = Edge(\n",
        "            from_=left_nfa.accept, to_=right_nfa.start, characters=EPSILON\n",
        "        )\n",
        "\n",
        "        # Finalize the Seq NFA and return it\n",
        "        states = [*left_nfa.states, *right_nfa.states]\n",
        "        transitions = [*left_nfa.transitions, *right_nfa.transitions, final_transition]\n",
        "        return NFA(left_nfa.start, right_nfa.accept, states, transitions)\n",
        "\n",
        "    def construct_star_plus_nfa(self, node):\n",
        "        # If we have regex A* or A+\n",
        "        is_star = isinstance(node, StarAstNode)\n",
        "\n",
        "        # Get the NFA representing A\n",
        "        left_nfa = self.construct_nfa(node.left)\n",
        "\n",
        "        # Create 2 new states, start and accept for the full NFA\n",
        "        start = State()\n",
        "        accept = State()\n",
        "\n",
        "        # Link the new start to the start of A's NFA\n",
        "        start_transition_1 = Edge(from_=start, to_=left_nfa.start, characters=EPSILON)\n",
        "\n",
        "        # For star nodes only, it also goes to the accept state directly to represent accepting an empty inputs\n",
        "        start_transition_2 = Edge(from_=start, to_=accept, characters=EPSILON)\n",
        "\n",
        "        # Link the accept state with the accept of A's NFA\n",
        "        final_transition_1 = Edge(\n",
        "            from_=left_nfa.accept, to_=accept, characters=EPSILON\n",
        "        )\n",
        "\n",
        "        # To represent zero|one 'or more' we link the accept state of A with the new start state\n",
        "        final_transition_2 = Edge(\n",
        "            from_=left_nfa.accept, to_=start, characters=EPSILON\n",
        "        )\n",
        "\n",
        "        # Finalize the star or plus NFA and return it\n",
        "        states = [*left_nfa.states, start, accept]\n",
        "        transitions = [\n",
        "            *left_nfa.transitions,\n",
        "            start_transition_1,\n",
        "            final_transition_1,\n",
        "            final_transition_2,\n",
        "        ]\n",
        "        if is_star:\n",
        "            transitions.append(start_transition_2)\n",
        "\n",
        "        return NFA(start, accept, states, transitions)\n",
        "\n",
        "    def construct_question_mark_nfa(self, node):\n",
        "        # If we have regex A?\n",
        "\n",
        "        # Get the NFA representing A\n",
        "        left_nfa = self.construct_nfa(node.left)\n",
        "\n",
        "        # Create 2 new states, start and accept for the full NFA\n",
        "        start = State()\n",
        "        accept = State()\n",
        "\n",
        "        # Link the new start to the start of A's NFA\n",
        "        start_transition_1 = Edge(from_=start, to_=left_nfa.start, characters=EPSILON)\n",
        "\n",
        "        # It also goes to the accept state directly to represent accepting an empty inputs\n",
        "        start_transition_2 = Edge(from_=start, to_=accept, characters=EPSILON)\n",
        "\n",
        "        # Link the accept state with the accept of A's NFA\n",
        "        final_transition_1 = Edge(\n",
        "            from_=left_nfa.accept, to_=accept, characters=EPSILON\n",
        "        )\n",
        "\n",
        "        # Finalize the question mark NFA and return it\n",
        "        states = [*left_nfa.states, start, accept]\n",
        "        transitions = [\n",
        "            *left_nfa.transitions,\n",
        "            start_transition_1,\n",
        "            start_transition_2,\n",
        "            final_transition_1,\n",
        "        ]\n",
        "\n",
        "        return NFA(start, accept, states, transitions)\n",
        "\n",
        "    def construct_literal_class_nfa(self, node):\n",
        "        # If we have regex 'x' for any character x\n",
        "\n",
        "        # Create 2 new states, start and accept for the full NFA\n",
        "        start = State()\n",
        "        accept = State()\n",
        "\n",
        "        # Single transition that goes from the starting to the accepting on the relevant characters\n",
        "        final_transition = Edge(start, accept, node.char)\n",
        "\n",
        "        # Finalize the literal and character class NFA and return it\n",
        "        states = [start, accept]\n",
        "        transitions = [final_transition]\n",
        "\n",
        "        return NFA(start, accept, states, transitions)\n",
        "\n",
        "    def AST_to_NFA(self):\n",
        "        self._nfa = self.construct_nfa(self._ast)\n",
        "\n",
        "    def nfa_to_json(self):\n",
        "        \"\"\"\n",
        "        This function is used to convert the NFA to a JSON format\n",
        "        \"\"\"\n",
        "        json_serialize = JsonSerialize()\n",
        "        self._nfa_json = json_serialize.nfa_json_serialize(self._nfa)\n",
        "        del json_serialize\n",
        "\n",
        "        # Store the nfa json in a file\n",
        "        with open(\"nfa.json\", \"w\") as f:\n",
        "            json.dump(self._nfa_json, f, indent=4)\n",
        "\n",
        "    def nfa_visualize(self, path=\"nfa\"):\n",
        "        graph_visualize = GraphVisualize()\n",
        "        if graph_visualize.graph_visualize(path, self._nfa_json):\n",
        "            print(f\"Visualization of the NFA is saved in {path}\")\n",
        "        else:\n",
        "            print(\"Error: Visualization failed\")\n",
        "        del graph_visualize\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ-w0lBzIQXS"
      },
      "source": [
        "### NFA to DFA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Qmm4Wym3JadD"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DfaState:\n",
        "    name: str = None\n",
        "\n",
        "    def __hash__(self) -> int:\n",
        "        return hash(self.name)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DfaEdge:\n",
        "    from_: DfaState\n",
        "    to_: DfaState\n",
        "    characters: set  # of chars (literals, epsilon) and pairs of chars (for ranges)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DFA:\n",
        "    start: DfaState\n",
        "    accept: list  # list of accepting states\n",
        "    non_accept: list  # list of non accepting states\n",
        "    # All states of the DFA including the start and accept\n",
        "    states: list  # of State\n",
        "    transitions: list  # of Edge\n",
        "\n",
        "\n",
        "class DFA_CLASS:\n",
        "    def __init__(self, dfa=None, inputs=[]):\n",
        "        \"\"\"_summary_\n",
        "        class that contains the methods to convert NFA to DFA\n",
        "        dfa: DFA object\n",
        "        inputs: list of inputs of the NFA\n",
        "        \"\"\"\n",
        "        self._dfa = dfa\n",
        "        self.inputs = inputs\n",
        "\n",
        "    def epsilon_closure(self, nfa, state):\n",
        "        \"\"\"_summary_\n",
        "        This function is used to get the epsilon closure of a given state\n",
        "        Args:\n",
        "            nfa (NFA): this a NFA object contains the NFA data\n",
        "            state (State): this is the state that we want to get the epsilon closure for\n",
        "\n",
        "        Returns:\n",
        "            set: closure set of states that are reachable from the given state using epsilon transitions\n",
        "        \"\"\"\n",
        "        closure = set()\n",
        "        # add the state to its closure set\n",
        "        closure.add(state)\n",
        "        for edge in nfa.transitions:\n",
        "            if edge.from_ == state and EPSILON in edge.characters:\n",
        "                # add the state that is reachable using epsilon transition to the closure set\n",
        "                closure.add(edge.to_)\n",
        "                # call the function recursively to get the epsilon closure of the next state\n",
        "                closure = closure.union(self.epsilon_closure(nfa, edge.to_))\n",
        "        return closure\n",
        "\n",
        "    def move(self, transitions, states, character):\n",
        "        \"\"\"_summary_\n",
        "        This function is used to get the move of a given set of states using a given character\n",
        "        Args:\n",
        "            transitions (List): list of transitions of the NFA (list of edges)\n",
        "            states (set): set of states that we want to get the move for. states list is a subset of nfa.states because we apply the input on only a subset\n",
        "            character (str): the character that we want to move to\n",
        "\n",
        "        Returns:\n",
        "            set: set of states that are reachable from the given states using the given character\n",
        "        \"\"\"\n",
        "        move = set()\n",
        "        for state in states:\n",
        "            for edge in transitions:\n",
        "                if edge.from_ == state and character in edge.characters:\n",
        "                    move.add(edge.to_)\n",
        "        return move\n",
        "\n",
        "    def subset_construction(self, nfa, inputs):\n",
        "        \"\"\"_summary_\n",
        "        This function is used to convert NFA to DFA using the subset construction algorithm\n",
        "        Args:\n",
        "            nfa (NFA): this a NFA object contains the NFA data\n",
        "            inputs (list): list of inputs of the NFA\n",
        "        \"\"\"\n",
        "        # get the epsilon closure of the start state\n",
        "        start_set = self.epsilon_closure(nfa, nfa.start)\n",
        "        # get the inputs of the NFA\n",
        "        self.inputs = inputs\n",
        "        # list of transitions of the DFA\n",
        "        transitions = []\n",
        "        # list of states of the DFA\n",
        "        states = []\n",
        "        # add the start state to the list of states of the DFA\n",
        "        states.append(start_set)\n",
        "        # list of accepting states of the DFA\n",
        "        accept = []\n",
        "        # list of non accepting states of the DFA\n",
        "        non_accept = []\n",
        "        # list of states that we need to visit\n",
        "        to_visit = [start_set]\n",
        "        # while there are states to visit\n",
        "        while to_visit:\n",
        "            # get the first state in the list of states to visit\n",
        "            current_state = to_visit.pop(0)\n",
        "            # for each input in the inputs list\n",
        "            for input_ in self.inputs:\n",
        "                # get the move of the current state using the input\n",
        "                move = self.move(nfa.transitions, current_state, input_)\n",
        "                # get the epsilon closure of the move set\n",
        "                for state in move:\n",
        "                    move = move.union(self.epsilon_closure(nfa, state))\n",
        "                # if the move set is not empty\n",
        "                if move:\n",
        "                    # add the move set to the list of states of the DFA\n",
        "                    if move not in states:\n",
        "                        states.append(move)\n",
        "                        to_visit.append(move)\n",
        "                    # add the transition from the current state to the move set using the input\n",
        "                    transitions.append(\n",
        "                        DfaEdge(from_=current_state, to_=move, characters={input_})\n",
        "                    )\n",
        "\n",
        "        # we need to convert every set of states to a state object\n",
        "        for i, state in enumerate(states):\n",
        "            temp = state\n",
        "            states[i] = DfaState(name=\",\".join([state.name for state in state]))\n",
        "            # loop over the transitions to update the from_ and to_ states\n",
        "            for transition in transitions:\n",
        "                if transition.from_ == temp:\n",
        "                    transition.from_ = states[i]\n",
        "                if transition.to_ == temp:\n",
        "                    transition.to_ = states[i]\n",
        "            # if the state contains an accepting state of the NFA\n",
        "            if temp.intersection({nfa.accept}):\n",
        "                # add the state to the list of accepting states of the DFA\n",
        "                accept.append(states[i])\n",
        "            else:\n",
        "                # add the state to the list of non accepting states of the DFA\n",
        "                non_accept.append(states[i])\n",
        "        # create a DFA object\n",
        "        self._dfa = DFA(\n",
        "            start=states[0],\n",
        "            accept=accept,\n",
        "            non_accept=non_accept,\n",
        "            states=states,\n",
        "            transitions=transitions,\n",
        "        )\n",
        "\n",
        "    def print_dfa(self):\n",
        "        \"\"\"\n",
        "        This function is used to print the DFA object\n",
        "        \"\"\"\n",
        "        print(\"Start State: \", self._dfa.start.name)\n",
        "        print(\"Accepting States: \", [state.name for state in self._dfa.accept])\n",
        "        print(\"Non Accepting States: \", [state.name for state in self._dfa.non_accept])\n",
        "        print(\"States: \", [state.name for state in self._dfa.states])\n",
        "        print(\"Transitions: \")\n",
        "        for transition in self._dfa.transitions:\n",
        "            print(\n",
        "                f\"From: {transition.from_.name}, To: {transition.to_.name}, Characters: {transition.characters}\"\n",
        "            )\n",
        "\n",
        "    def rename_dfa_states(self):\n",
        "        \"\"\"\n",
        "        This function is used to rename the states of the DFA to S0, S1, S2, ...\n",
        "        \"\"\"\n",
        "        for i, state in enumerate(self._dfa.states):\n",
        "            state.name = f\"S{i}\"\n",
        "\n",
        "    def dfa_to_json(self, file_path: str):\n",
        "        \"\"\"\n",
        "        This function is used to convert the DFA to a JSON format (serialize the DFA object)\n",
        "        Args:\n",
        "            file_path (str): the path of the file that we want to store the JSON data in\n",
        "        Returns:\n",
        "            None: no return value\n",
        "        \"\"\"\n",
        "        json_serialize = JsonSerialize()\n",
        "        self.dfa_json = json_serialize.dfa_json_serialize(self._dfa)\n",
        "        del json_serialize\n",
        "        # store the dfa json in a file\n",
        "        with open(file_path, \"w\") as f:\n",
        "            json.dump(self.dfa_json, f, indent=4)\n",
        "\n",
        "    def visualize_dfa(self, path=\"./dfa\"):\n",
        "        \"\"\"_summary_\n",
        "        Args:\n",
        "            path (str, optional): Path where the gv file will be stores. Defaults to \"./dfa.gv\".\n",
        "        \"\"\"\n",
        "        graph_visualize = GraphVisualize()\n",
        "        if graph_visualize.graph_visualize(path, self.dfa_json):\n",
        "            print(f\"Visualization of the DFA is saved in {path}\")\n",
        "        else:\n",
        "            print(\"Error: Visualization DFA failed\")\n",
        "        del graph_visualize\n",
        "\n",
        "    def minimize_dfa(self, inputs) -> DFA:\n",
        "        \"\"\"_summary_\n",
        "        This function is used to minimize the DFA\n",
        "        Args:\n",
        "            inputs (list): list of inputs of the NFA (Terminals in Regex)\n",
        "        Returns:\n",
        "            DFA: minimized DFA object\n",
        "        \"\"\"\n",
        "        # the current state of the DFA\n",
        "        pi = []\n",
        "        # list of accepting states of the DFA\n",
        "        accept = self._dfa.accept\n",
        "        if accept:\n",
        "            # add the accepting states to the current state of the DFA\n",
        "            pi.append(set(accept))\n",
        "        # list of non accepting states of the DFA\n",
        "        non_accept = self._dfa.non_accept\n",
        "        if non_accept:\n",
        "            # add the non accepting states to the current state of the DFA\n",
        "            pi.append(set(non_accept))\n",
        "        print(\"Initial Partition: \", pi)\n",
        "        # flag to tell if there is a change in the partition\n",
        "        change = True\n",
        "        while change:\n",
        "            change = False\n",
        "            # create a new partition\n",
        "            new_pi = []\n",
        "            for group in pi:\n",
        "                if len(group) > 1:\n",
        "                    # the splited states in the group\n",
        "                    # key is the transition table for the state\n",
        "                    splitted_states = {}\n",
        "                    for state in group:\n",
        "                        # transion table for each state\n",
        "                        state_transition_table = {}\n",
        "                        for input_ in inputs:\n",
        "                            for transition in self._dfa.transitions:\n",
        "                                if (\n",
        "                                    transition.from_ == state\n",
        "                                    and input_ in transition.characters\n",
        "                                ):\n",
        "                                    if input_ not in state_transition_table:\n",
        "                                        state_transition_table[input_] = set()\n",
        "                                    state_transition_table[input_].add(transition.to_)\n",
        "                        str_temp = str(state_transition_table)\n",
        "                        print(\"Transition Table: \", str_temp)\n",
        "                        if str_temp not in splitted_states:\n",
        "                            splitted_states[str_temp] = set()\n",
        "                        splitted_states[str_temp].add(state)\n",
        "                    # print(\"Splitted States: \", splitted_states)\n",
        "                    # if there is more than one transition table for the group\n",
        "                    if len(splitted_states) > 1:\n",
        "                        # change is True to check on the new partitions\n",
        "                        change = True\n",
        "                        print(\"Splitted States: \", splitted_states)\n",
        "                        # add the splited states to the new partition\n",
        "                        for value in splitted_states.values():\n",
        "                            new_pi.append(set(value))\n",
        "                    else:\n",
        "                        # if there is only one transition table for the group\n",
        "                        # add the group to the new partition\n",
        "                        new_pi.append(group)\n",
        "                else:\n",
        "                    # if the group contains only one state\n",
        "                    # add the group to the new partition\n",
        "                    new_pi.append(group)\n",
        "            # if there is a change in the partition\n",
        "            if change:\n",
        "                pi = new_pi\n",
        "                # print(\"New Partition: \", pi)\n",
        "        # list of states of the minimized DFA\n",
        "        min_states = []\n",
        "        # list of transitions of the minimized DFA\n",
        "        min_transitions = []\n",
        "        # list of accepting states of the minimized DFA\n",
        "        min_accept = []\n",
        "        # list of non accepting states of the minimized DFA\n",
        "        min_non_accept = []\n",
        "        # start state of the minimized DFA\n",
        "        start = None\n",
        "        print(\"Final Partition: \", pi)\n",
        "        # create the state sof the minimized DFA\n",
        "        for i, group in enumerate(pi):\n",
        "            state = DfaState(name=f\"S{i}\")\n",
        "            min_states.append(state)\n",
        "            if group.intersection(self._dfa.accept):\n",
        "                min_accept.append(state)\n",
        "            else:\n",
        "                min_non_accept.append(state)\n",
        "            # get the start state of the minimized DFA\n",
        "            if self._dfa.start in group:\n",
        "                start = state\n",
        "        # create the transitions of the minimized DFA\n",
        "        for transition in self._dfa.transitions:\n",
        "            for i, group in enumerate(pi):\n",
        "                if transition.from_ in group:\n",
        "                    # note that the number of groups in pi is the number of states in min_states so each group maps to a state\n",
        "                    from_ = min_states[i]\n",
        "                if transition.to_ in group:\n",
        "                    to_ = min_states[i]\n",
        "            min_transitions.append(\n",
        "                DfaEdge(from_=from_, to_=to_, characters=transition.characters)\n",
        "            )\n",
        "        # create a DFA object for the minimized DFA\n",
        "        minimized_dfa = DFA(\n",
        "            start=start,\n",
        "            accept=min_accept,\n",
        "            non_accept=min_non_accept,\n",
        "            states=min_states,\n",
        "            transitions=min_transitions,\n",
        "        )\n",
        "        return minimized_dfa\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD9NUWtkHSKZ"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YZ-6NRnJGatQ"
      },
      "outputs": [],
      "source": [
        "def json_to_nfa(json_path: str) -> NFA:\n",
        "    \"\"\"\n",
        "    This function is used to convert the JSON data to NFA\n",
        "    params:\n",
        "        json_path: str: Path to the JSON file\n",
        "    return:\n",
        "        NFA: NFA object\n",
        "    \"\"\"\n",
        "    # read the JSON file\n",
        "    with open(json_path, \"r\") as file:\n",
        "        json_data = json.load(file)\n",
        "    # create the NFA object\n",
        "    nfa = NFA(None, None, [], [])\n",
        "    # create the states\n",
        "    states = []\n",
        "    acceping_index = 0\n",
        "    for state in json_data:\n",
        "        if state == \"startingState\":\n",
        "            continue\n",
        "        if json_data[state][\"isTerminatingState\"] == True:\n",
        "            acceping_index = int(state[1:])\n",
        "        states.append(DfaState(name=state))\n",
        "    nfa.states = states\n",
        "    # set the start state becuase the name of the starting states is the index of it inside the states array\n",
        "    nfa.start = states[int(json_data[\"startingState\"][1:])]\n",
        "    # set the accept state\n",
        "    nfa.accept = states[acceping_index]\n",
        "    # create the transitions\n",
        "    transitions = []\n",
        "    for state in json_data:\n",
        "        if state == \"startingState\":\n",
        "            continue\n",
        "        for transition in json_data[state]:\n",
        "            if transition == \"isTerminatingState\":\n",
        "                continue\n",
        "\n",
        "            # check if the transition is epsilon or not\n",
        "            if transition.startswith(\"epsilon\"):\n",
        "                transitions.append(\n",
        "                    DfaEdge(\n",
        "                        states[int(state[1:])],\n",
        "                        states[int(json_data[state][transition][1:])],\n",
        "                        {EPSILON},\n",
        "                    )\n",
        "                )\n",
        "            else:\n",
        "                transitions.append(\n",
        "                    DfaEdge(\n",
        "                        states[int(state[1:])],\n",
        "                        states[int(json_data[state][transition][1:])],\n",
        "                        {transition},\n",
        "                    )\n",
        "                )\n",
        "    nfa.transitions = transitions\n",
        "    return nfa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kkmGA4zOHhAr"
      },
      "outputs": [],
      "source": [
        "# getting the inputs from the tansitions in the nfa\n",
        "def get_inputs(transitions):\n",
        "    \"\"\"_summary_\n",
        "    This function is used to get the inputs from the transitions in the NFA\n",
        "    Args:\n",
        "        transitions (List): List of transitions in the NFA (List of Edges)\n",
        "    Returns:\n",
        "        List: List of unique inputs in the NFA\n",
        "    \"\"\"\n",
        "    inputs = []\n",
        "    for transition in transitions:\n",
        "        temp_in = list(transition.characters)[0]\n",
        "        if temp_in not in inputs and temp_in != EPSILON:\n",
        "            inputs.append(temp_in)\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VsAtoLhJn1V"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vF2qaye-JnYb",
        "outputId": "cc07aac5-bc04-404e-e2e2-928fed6f005f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the regular expression: as\n",
            "Visualization of the NFA is saved in nfa\n",
            "Visualization of the DFA is saved in ./dfa\n",
            "Initial Partition:  [{DfaState(name='S2')}, {DfaState(name='S0'), DfaState(name='S1')}]\n",
            "Transition Table:  {'a': {DfaState(name='S1')}}\n",
            "Transition Table:  {'s': {DfaState(name='S2')}}\n",
            "Splitted States:  {\"{'a': {DfaState(name='S1')}}\": {DfaState(name='S0')}, \"{'s': {DfaState(name='S2')}}\": {DfaState(name='S1')}}\n",
            "Final Partition:  [{DfaState(name='S2')}, {DfaState(name='S0')}, {DfaState(name='S1')}]\n",
            "Start State:  S1\n",
            "Accepting States:  ['S0']\n",
            "Non Accepting States:  ['S1', 'S2']\n",
            "States:  ['S0', 'S1', 'S2']\n",
            "Transitions: \n",
            "From: S1, To: S2, Characters: {'a'}\n",
            "From: S2, To: S0, Characters: {'s'}\n",
            "Visualization of the DFA is saved in minimized_dfa\n"
          ]
        }
      ],
      "source": [
        "input_regex = input(\"Enter the regular expression: \")\n",
        "# input_regex = \"[a-z]*[0-9]+\"\n",
        "nfa = NFA_CLASS(input_regex)\n",
        "deserialized_nfa = json_to_nfa(\"./nfa.json\")\n",
        "inputs = get_inputs(deserialized_nfa.transitions)\n",
        "dfa = DFA_CLASS()\n",
        "dfa.subset_construction(deserialized_nfa, inputs)\n",
        "dfa.rename_dfa_states()\n",
        "dfa.dfa_to_json(\"dfa.json\")\n",
        "dfa.visualize_dfa()\n",
        "# dfa.print_dfa()\n",
        "minimized_dfa = dfa.minimize_dfa(inputs)\n",
        "minimized_dfa_obj = DFA_CLASS(minimized_dfa,inputs)\n",
        "minimized_dfa_obj.print_dfa()\n",
        "minimized_dfa_obj.dfa_to_json(\"minimized_dfa.json\")\n",
        "minimized_dfa_obj.visualize_dfa(path=\"minimized_dfa\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UTNLz7DyJqlr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
